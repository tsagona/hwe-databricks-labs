{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 2 Lab: File Formats, Delta Tables, and Time Travel\n\nIn this lab you will explore the differences between CSV and Delta Lake (Parquet) storage.\n\nYou'll see firsthand how:\n- **Columnar storage** (Parquet/Delta) dramatically reduces I/O for analytical queries\n- **File size** shrinks 10\u201320x thanks to Parquet's compression techniques\n- **Time travel** lets you query, audit, and restore previous versions of a table"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Prerequisites\n\nBefore running this notebook, run the `generate_sensor_data` notebook to create a CSV file at\n`/FileStore/hwe-data/week2/sensor_readings/` (10 million rows)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 1: Create Schema\n\nWe'll use a `week2` schema to hold everything we build in this lab."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CREATE SCHEMA IF NOT EXISTS week2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 2: Explore the CSV Data\n\nFirst, let's look at the raw CSV file using `read_files`. This is how Databricks reads\nnon-Delta files \u2014 it parses the CSV on every query."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Preview the first few rows. Notice Databricks must read and parse the entire CSV to return even a few rows."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT *\nFROM read_files(\n  '/FileStore/hwe-data/week2/sensor_readings',\n  format => 'csv',\n  header => true\n)\nLIMIT 10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Check the schema that Databricks inferred from the CSV. Notice that everything is STRING \u2014\nCSV has no type information embedded in the file."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DESCRIBE SELECT *\nFROM read_files(\n  '/FileStore/hwe-data/week2/sensor_readings',\n  format => 'csv',\n  header => true\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 3: Create a Delta Table from CSV\n\nLet's create a Delta table from the CSV using CREATE TABLE AS SELECT (CTAS).\nThis converts the CSV data into Delta format (Parquet files + transaction log)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE week2.sensor_readings AS\nSELECT\n  sensor_id,\n  sensor_type,\n  location,\n  CAST(reading_timestamp AS TIMESTAMP) AS reading_timestamp,\n  CAST(reading_value AS DOUBLE) AS reading_value,\n  unit,\n  CAST(battery_pct AS INT) AS battery_pct,\n  CAST(signal_strength AS INT) AS signal_strength,\n  status,\n  firmware_version,\n  CAST(deployed_date AS DATE) AS deployed_date,\n  CAST(maintenance_flag AS BOOLEAN) AS maintenance_flag\nFROM read_files(\n  '/FileStore/hwe-data/week2/sensor_readings',\n  format => 'csv',\n  header => true\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now compare the Delta table's schema to the CSV. The Delta table preserved the original types\n(TIMESTAMP, DOUBLE, INT, BOOLEAN, DATE) because Delta stores schema metadata alongside the data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DESCRIBE week2.sensor_readings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 4: Compare File Sizes\n\nDelta tables store data as Parquet files, which use columnar compression:\n- **Dictionary encoding** for low-cardinality strings (sensor_type has only 5 values \u2192 stored as integer codes)\n- **Delta encoding** for sequential values (timestamps increase by 1 second \u2192 store only the difference)\n- **Bit packing** for small integers (battery_pct 0\u2013100 needs only 7 bits, not 32)\n- **Run-length encoding** for repeated values (maintenance_flag is mostly false \u2192 store \"false \u00d7 N\")\n\nCSV stores everything as plain text with no compression at all."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Check the Delta table's size on disk using `DESCRIBE DETAIL`. Look at the `sizeInBytes` column."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DESCRIBE DETAIL week2.sensor_readings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Count the rows to confirm both formats have the same data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT\n  (SELECT COUNT(*) FROM week2.sensor_readings) AS delta_row_count,\n  (SELECT COUNT(*)\n   FROM read_files(\n     '/FileStore/hwe-data/week2/sensor_readings',\n     format => 'csv',\n     header => true\n   )\n  ) AS csv_row_count"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 5: Compare Query Performance\n\nThis is where the difference really shows. We'll run the same queries against CSV and Delta,\nthen compare how much data each query had to read.\n\n### How to view I/O statistics\n\nAfter each query runs, look at the bottom of the cell output for the execution time\n(e.g., \"Took 2.34 seconds\"). Click on it to open the **Query Profile**.\n\nIn the Query Profile, click on the **Scan** operator (the box at the bottom of the diagram).\nThe right panel will show:\n- **data read size** \u2014 how many bytes were read from storage\n- **rows read** \u2014 how many rows were scanned\n\nCompare these numbers between the CSV and Delta versions of each query."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test 1: Aggregate a single column\n\nThis query only needs the `reading_value` column. The Delta/Parquet version can skip\nall 11 other columns entirely \u2014 it only reads the one it needs. The CSV version must\nread and parse every column of every row."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**CSV** \u2014 Run this, then open the Query Profile and note the data read size."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT AVG(CAST(reading_value AS DOUBLE)) AS avg_reading\nFROM read_files(\n  '/FileStore/hwe-data/week2/sensor_readings',\n  format => 'csv',\n  header => true\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Delta** \u2014 Run this, then compare the data read size to the CSV version above."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT AVG(reading_value) AS avg_reading\nFROM week2.sensor_readings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test 2: Filter and aggregate two columns\n\nThis query filters on `sensor_type` and aggregates `reading_value`. With Parquet,\nDatabricks reads only these two columns. With CSV, it reads everything."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**CSV**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT\n  sensor_type,\n  COUNT(*) AS readings,\n  ROUND(AVG(CAST(reading_value AS DOUBLE)), 2) AS avg_value\nFROM read_files(\n  '/FileStore/hwe-data/week2/sensor_readings',\n  format => 'csv',\n  header => true\n)\nGROUP BY sensor_type\nORDER BY sensor_type"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Delta**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT\n  sensor_type,\n  COUNT(*) AS readings,\n  ROUND(AVG(reading_value), 2) AS avg_value\nFROM week2.sensor_readings\nGROUP BY sensor_type\nORDER BY sensor_type"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test 3: Multi-column analytical query\n\nA more realistic query that joins several columns. Even here, Delta reads far less data\nbecause it only reads the 4 columns referenced in the query, not all 12."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**CSV**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT\n  sensor_type,\n  status,\n  COUNT(*) AS readings,\n  ROUND(AVG(CAST(battery_pct AS INT)), 1) AS avg_battery\nFROM read_files(\n  '/FileStore/hwe-data/week2/sensor_readings',\n  format => 'csv',\n  header => true\n)\nGROUP BY sensor_type, status\nORDER BY sensor_type, status"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Delta**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT\n  sensor_type,\n  status,\n  COUNT(*) AS readings,\n  ROUND(AVG(battery_pct), 1) AS avg_battery\nFROM week2.sensor_readings\nGROUP BY sensor_type, status\nORDER BY sensor_type, status"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### What you should see\n\n| Metric | CSV | Delta |\n|--------|-----|-------|\n| Data read size (Test 1) | ~full file size | ~1/12 of Delta size |\n| Data read size (Test 2) | ~full file size | ~2/12 of Delta size |\n| Query time | Slower | Faster |\n\nThe CSV queries always read the entire file regardless of which columns you select.\nDelta/Parquet reads only the columns your query references \u2014 this is called **column pruning**.\n\nCombined with compression, Delta typically reads 10\u201350x less data than CSV for analytical queries."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 6: Time Travel\n\nDelta Lake maintains a transaction log that records every change to a table.\nThis means you can:\n- **Query previous versions** of the data\n- **See the history** of all operations\n- **Restore** to a previous version if something goes wrong"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "First, let's check the current state of the table. We'll look at a specific sensor to\nmake the changes easy to track."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT sensor_id, sensor_type, reading_value, status\nFROM week2.sensor_readings\nWHERE sensor_id = 'sensor-0001'\nLIMIT 5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Check the table's version history before making changes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DESCRIBE HISTORY week2.sensor_readings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now let's make a change \u2014 update all readings for sensor-0001 to set their status to `'maintenance'`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "UPDATE week2.sensor_readings\nSET status = 'maintenance'\nWHERE sensor_id = 'sensor-0001'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Verify the update took effect."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT sensor_id, sensor_type, reading_value, status\nFROM week2.sensor_readings\nWHERE sensor_id = 'sensor-0001'\nLIMIT 5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Check the history again \u2014 you should see a new version from the UPDATE operation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DESCRIBE HISTORY week2.sensor_readings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Query a previous version\n\nUse `VERSION AS OF` to read the data as it was before the update.\nVersion 0 is the original table as it was first written."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT sensor_id, sensor_type, reading_value, status\nFROM week2.sensor_readings VERSION AS OF 0\nWHERE sensor_id = 'sensor-0001'\nLIMIT 5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The original status values are still there in version 0, even though the current\nversion shows `'maintenance'`. Delta kept the old Parquet files and just recorded\nwhich files belong to which version in the transaction log."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Restore to a previous version\n\nIf the UPDATE was a mistake, we can undo it by restoring the table to version 0."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "RESTORE TABLE week2.sensor_readings TO VERSION AS OF 0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Verify the restore worked \u2014 the status values should be back to their original values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SELECT sensor_id, sensor_type, reading_value, status\nFROM week2.sensor_readings\nWHERE sensor_id = 'sensor-0001'\nLIMIT 5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Check history one more time \u2014 you'll see the RESTORE as yet another version in the log.\nNothing is ever truly lost with Delta Lake."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DESCRIBE HISTORY week2.sensor_readings"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Step 7: Clean Up\n\nDrop the `week2` schema and all its tables."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DROP SCHEMA IF EXISTS week2 CASCADE"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
